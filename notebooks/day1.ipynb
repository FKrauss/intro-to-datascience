{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1\n",
    "\n",
    "## Module 2\n",
    "\n",
    "In this module you will learn how to load the online retail dataset in Python, visualise and summarise it to produce insights that will guide your machine learning endevours in the next modules. We will learn how to plot data and calculate summary statistics to build a dataset from which we ultimately will try to predict future customer behaviour.\n",
    "\n",
    "### Learning Activity - Loading Libraries\n",
    "\n",
    "First we need to load the required Python libraries. Libraries are like extensions to the base `python` that add functionality or help to make tasks more convenient to do. We will load some libraries that will boost your data handling capacity.\n",
    "\n",
    "The main ones include `numpy` and `pandas`, which are the most prominent libraries to work efficiently with data in python. Here we just use the `import` function to, you guessed it import the pandas library and make it accessible `as` `pd` in the following code to save some typing (4 characters to be precise...). Then we load `matplotlib` and `seaborn` which are libraries that will help you to visualise the data. Visualisation of a dataset is key to getting a good understanding of what it is made before applying more involved machine learning algorithms. You will learn how handy it is to start formulating hypotheses and to evaluate output from data processing you will be doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "\n",
    "# Module 2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Module 3\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Additional plotting functionality \n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D \n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "init_notebook_mode()\n",
    "%matplotlib inline\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "print(\"libraries all imported, ready to go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "The dataset is from a online retailer selling gifts and is based on a dataset taken from [here](https://archive.ics.uci.edu/ml/datasets/Online+Retail#).\n",
    "\n",
    "![Giftshop](img/giftshop.jpg)\n",
    "\n",
    "It is a transaction history of an online shop and as we will load it into python we will see that it comes with a set of feature descriptions:\n",
    "\n",
    "* `InvoiceNo`: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "* `StockCode`: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "* `Description`: Product (item) name. Nominal.\n",
    "* `Quantity`: The quantities of each product (item) per transaction. Numeric.\n",
    "* `InvoiceDate`: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "* `UnitPrice`: Unit price. Numeric, Product price per unit in sterling.\n",
    "* `CustomerID`: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "* `Country`: Country name. Nominal, the name of the country where each customer resides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Loading the dataset\n",
    "\n",
    "In a first step we load the dataset with `pandas`. To achieve this you will use the `.read_csv()` method. We just need to point to the location of the dataset and indicate under what name we want to store the data, i.e. `retail`, and `pandas` will do the rest. In the `read_csv()` function, the `parse_dates` parameter is a boolean or list of ints or names or list of lists or dict, which by default is set to False. In this case, we are passing the name 'InvoiceDate' that represents the corresponding column. More details on how to use `parse_dates` can be found http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html. \n",
    "\n",
    "At a first stage, the data has only been loaded. Let's have a look at the top few lines - we can use the `.head()` method to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the data and explore the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We already learned one thing, customer `17850` bought 6 `WHITE METAL LANTERN`s on the 1st of December in the early morning. Perhaps a Christmas present?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also good practice to **always** check the dimensionality of the input data using the `shape` command to confirm that you really have imported all the data in the correct way and format (e.g. one common mistake is to get the separator wrong and end up with only one column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the dimensionality of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "At this stage we do not really know what is going on in this dataset. We need to get beyond the first impression. How about trying to answer some simple questions like:\n",
    "\n",
    "* How many customers are we dealing with?\n",
    "* How many different products are being sold?\n",
    "* What country spends how much?\n",
    "* What has been the company's profit during the last year?\n",
    "* What period does the data span?\n",
    "\n",
    "We will go through these questions and learn new tricks as we move along. But before we get started, it is worth to have a little bit more background on what `pandas` is made of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data selection with Pandas\n",
    "\n",
    "Throughout this bootcamp you will be using `pandas` which is a python library that makes it muuuuuch more convenient to work with data than the base `python` methods. `pandas` is built on top of `numpy`, which is the library that brings efficient numerical operations to `python`. As the author of `pandas` Wes McKinney puts it: '*pandas provides high level data manipulation tools built on top of NumPy*'. `pandas` takes care of making it easy to work with tabular data in providing selections, merging, calculating statistics, filling in missing values and provides solutions to many other challenges that would be cumbersome to overcome with base `python`.\n",
    "\n",
    "When you load data into python with `pandas` it is put into a special structure called a `DataFrame`. `DataFrame`s are what makes `pandas` so convenient to work with for data analysis. It is worth to take the time to understand what kind of an object a `DataFrame` is, or in other words what it is made of in order to get all the benefits it has on offer.\n",
    "\n",
    "![Table Anatomy Class](./img/online_retail_table_anatomy.png)\n",
    "\n",
    "You are familiar with what a table is, it has column and rows and often these are annotated with column labels and row labels respectively. But how are table encoded with `pandas`. The raw data is stored in `numpy` **arrays** and this is where `pandas` can leverage all the numerical data processing.\n",
    "\n",
    "To add more convenient selection on top of this array, it is encoded in a so-called `pd.Series`, which can be thought of as a table with a single column. Crucially a `pd.Series` can have a (column) label and row labels. Also a `pd.Series` will store data of a given type, i.e. numbers, words, times. Row labels are called indexes in `pandas` and they are very important for a lot of `pandas` and we will introduce some later in the module. The `pd.Series` comes with many of the convenience functions that are included in `numpy`, such as `.sum()`, `.max()` etc. However, is has some additional functionality that `numpy` is missing. For instance it is very easy to count the unique number of entires in a pd.Series by simply using the `.nunique()` method.\n",
    "\n",
    "Finally a bunch of `pd.Series` in one table constitute the `DataFrame`, with column labels and an index. Crucially, `DataFrame`s can have different types of data in different columns, which is essential when representing tables. Within a `DataFrame`, the different columns of the table can easily be accessed via the *name* of the columns. Similarly, you can select individual rows via the indices.\n",
    "\n",
    "In this primer you will go though a lot of the basic pandas functionality and try to understand how they are build so it will be easier to maniputate them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - How many?\n",
    "\n",
    "Remember one of the questions from before? How many customers are we dealing with? Let's have a look.\n",
    "\n",
    "First we need to select the `CustomerID` column, which we do with the square brackets (`[]`). This yields a `pd.Series` only containing `CustomerID` column. From this we can then count the unique values with the `.unique()` function that is conventiently provided for `pd.Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply the nunique() function on the column 'CustomerID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Activity \n",
    "\n",
    "Let's repeat this to find out how many countries the customers are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": true
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply the nunique() function on the column 'Country'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find out how many different products the shop is keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": true
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply the nunique() function on the column 'StockCode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Activity\n",
    "\n",
    "Perhaps you have noticed that repeating the same operation again and again can become annoying. Let's be more efficient here.\n",
    "\n",
    "The remedy is `pandas`' `.apply()` method, which allows you to apply a function on all the columns with a one line command. To do this we need to just select `retail` `Dataframe` and specify what function we want to apply. Before we were always using the `.nunique()` method, so let's do the same. In this context you need to be a bit more precise on where that function is form. As mentioned before, the `.nunique()` method works on pd.Series and that is how you will find it under `pd.Series.nunique`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the count of unique values for all the columns using the apply() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cca": {
     "exercise": false
    }
   },
   "source": [
    "The `apply()` function will automatically go through all the columns and return the number of unique values in each column. We will be relying on the `.apply()` method later on when we calculate features from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - A customer's basket\n",
    "\n",
    "You should have a look at a transaction in more detail to see how it is structured. Let's say we were interesting in having a closer look at the invoice with `InvoiceNo` `544182`. How would you filter just the rows of the `retail` table that has records on that specific invoice?\n",
    "\n",
    "To do this we need to learn how to filter the dataset with `pandas`. The syntax for filtering is a bit cumbersome but it makes a lot of sense once you get the hang of it. To get it you need to understand that you can select from a `pd.DataFrame` in providing a `pd.Series` of `True` and `False` values. When doing so, `pandas` will only return the rows of the `pd.DataFrame` where `pd.Series` was True and will discard the rest.\n",
    "\n",
    "Thus filtering a table is a two step process: \n",
    "\n",
    "* you build a `pd.Series` that indicates the rows that fit your condition\n",
    "* then you select these rows from the original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter out and display the rows for the invoice number 544182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cca": {
     "exercise": false
    }
   },
   "source": [
    "Filtering can also be used to select for rows that are in a give numberic range. For instance try to filter the `invoice` `DataFrame` for just those items with a `UnitPrice` above £5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": true
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter out all invoices where the 'UnitPrice' is above 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Activity\n",
    "\n",
    "How much did the customer spend in total? From our dataset two columns will be very helpful to calculate the total send, i.e. `Quantity`, the number of items that were bought and `UnitPrice`, the amount each item cost. If we multiply these columns with each other we get how much the customer spent per item type and we only need to sum up the cost for all items.\n",
    "\n",
    "Let's start with the total price per item. With `pandas` this is straight forward as we can simply select columns and multiply them with each other. Make sure to store the resulting `pd.Series` in a variable. Then we calculate the sum of all the item prices using the `pandas` method `.sum()`; remember that we can do this because the data is stored in a `pd.Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": true
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start by multiplying the invoices' 'Quantity' by the 'UnitPrice' and then sum everything up with .sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go, this customer spent a total of £850 in this invoice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cca": {
     "exercise": false
    }
   },
   "source": [
    "What is the total turnover of the retail store. In essance it is the same calculation, but be careful to use the right `pd.Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the total turnover of the retail store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - A customer's history\n",
    "\n",
    "Knowing the total spend for one invoice is good, but this will not teach us much about this customer in general. Let's also look at all the transactions of that customer (`CustomerID` `18257`).\n",
    "\n",
    "You will first have to select all the rows in the table that are associated with the customer vis the `CustomerID` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": true
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter out and display the rows for the customer 18257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many purchases did this customer do? You should be able to do this with what we learned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": true
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the number of unique invoices for the customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the total spent for purchase (invoice) '563569' of this customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter out the invoices with InvoiceID equal to '563569'\n",
    "\n",
    "# Find the product cost \n",
    "\n",
    "# Calculate the total cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that repeating these operations can become quite cumbersome. We should write a function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 2 - Writing simple functions\n",
    "\n",
    "`python` contains a whole range of predefined functions that makes is so useful, and we have been using some. Missing functionality is quickly added when loading libraries and so far we have mostly relied on such functions. Here we will learn how to write our own simple functions that help us to simplify our tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some simple pseudo code of a function. Generally speaking a function will take some input, for example some `data` on which it performs an operation. In this case the simple `.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 'def' defines the function where simple_sum is its name and data is a parameter\n",
    "\n",
    "def simple_sum(data):\n",
    "    output = data.sum()  # here the body of the function starts and its operation is performed\n",
    "    return output  # finally the function needs to return the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cca": {
     "exercise": false
    }
   },
   "source": [
    "At this stage we have defined the function. In order to use it, we need to call it in a way that should be very familiar to you already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Call the 'simple_sum' function specifying the 'data' parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cca": {
     "exercise": false
    }
   },
   "source": [
    "This function is very simple and pointless really as we could simply have done `invoice['UnitPrice'].sum()`. However you can change the crucial part of the function to anything you like, i.e. the portion where we perform an operation on the data.\n",
    "\n",
    "In this way functions offer you to extend the functionality of python. Remeber until now you have been using many predefined python functions that made your life easier. The functions that you will write in the follwing exercises are for exactly the same purpose, to make your life easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - A customer's history at high throughput\n",
    "\n",
    "What if we want to know the total spent by the customer for each purchase in a more systematic way? So far we had to repeat this operation for every purchase of the customer. This will be a two step process:\n",
    "\n",
    "* first we group the data on a per-invoice-basis\n",
    "* then we apply a function that calculates the total spend per group\n",
    "\n",
    "This is a very commonly used framework to calculate summary statistics from data. In the following part of the notebook, we will apply this strategy to calculate the summary statistics for the total spent per invoice, and per country and per time period.\n",
    "\n",
    "For the grouping, we can use a very handy function of `pandas` allows to go though the data on a per-invoices basis. Enter the `.groupby()` method.\n",
    "\n",
    "![Group by operation](./img/online_retail_groupby.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby InvoiceNo\n",
    "The group `.groupby()` call is very simple, we just need to specify the column that we would want to group by. In this case it is the `InvoiceNo` (Note: you can also group by multiple columns and we will see that a bit later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Group the customers history by its 'InvoiceNo's and display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the background the `InvoiceNo` column was converted into a sorted index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the function\n",
    "\n",
    "After grouping we can then apply an operation to each to and `.apply()`. Let's first write a simple function that calcualtes the total spent, inspired with what we have already done in a previous learning activity.\n",
    "\n",
    "We first need to define the functions with...\n",
    "\n",
    "    def total_spend(data):\n",
    "        ---> operation goes here <---\n",
    "        return pd.Series({'column_name': column_values)\n",
    "    \n",
    "...where `data` is the data input and we `return` a `pd.Series` where we have set a `column_name` (Note: You could also simply return the the `column_values` without passing them in a `pd.Series` object, but then you would lose the option to give a `column_name`).\n",
    "\n",
    "Your task now is just to build a function that fits in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a function called 'total_spent' that takes 'data' as a parameter\n",
    "# where the operation calculates the amount spent per product. Remember\n",
    "# that you can do 'Quantity' time 'UnitPrice'). Finally '.sum()' the output\n",
    "# and return it as a pd.Series.\n",
    "\n",
    "def total_spent(data):\n",
    "    product_spend = data['Quantity'] * data['UnitPrice']\n",
    "    total_spent   = product_spend.sum()\n",
    "    return pd.Series({'total_spent': total_spent})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we have defined the function and it hosts exactly the same operation as in a previous learning function, with the only difference that it is contained withing a neat function.\n",
    "\n",
    "We can now `.apply()` it to our grouped `pd.DataFrame`.\n",
    "\n",
    "![Apply the total_spent function](img/online_retail_apply_total_spent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply the total_spent() function on the grouped customers\n",
    "\n",
    "# Reset the index\n",
    "\n",
    "# Display the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can confirm from above where you calculated the two invoices (namely `544182` and `563569`), we get the same results. Crucially, we now have all the customer's total spends and we even know how many times the customer instigated a purchase. The result is presented in a neat pd.Series.\n",
    "\n",
    "We can still do better in resetting the index that was set during the `.groupby()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus activity\n",
    "\n",
    "Can you sort the output of the `pd.DataFrame`, to make the table easier to read?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sort the values of total_spent in an increasing order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Tables to plots\n",
    "\n",
    "Tables are a visual way (albeit primitive) to look into a dataset. There is much more that we can do to simplify getting insights from the data and to communicated it clearly to others.\n",
    "\n",
    "We will be using the `seaborn` library for most plots in this bootcamp. It is based on the `python` classic `matplotlib`, which we loaded earlier. In a nutshell we define plot objects that can have a variety of properties, e.g. type of plots, dataset, data mapped to x-axis etc. Then we use `seaborn`'s helper functions that take care of most of the plotting setup to produce a graph where many reasonable presets have been set.\n",
    "\n",
    "In this way there is a whole range of plot types that can be quickly produced. Have a look [here](https://stanford.edu/~mwaskom/software/seaborn/examples/index.html) for a gallery of plots possible with `seaborn`.\n",
    "\n",
    "![Seaborn gallery](img/seaborn_gallery.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's produce our first plot.\n",
    "\n",
    "To do this we first initiate a figure object. Here we can set some parameters such as figure dimentsions, but there are many more to pick from [here](http://matplotlib.org/api/figure_api.html). At this stage we have an emply plot.\n",
    "\n",
    "When we build a plot we need to ask yourself three things:\n",
    "\n",
    "* What data do I want to plot?\n",
    "* What type of plot is suited for the data?\n",
    "* What aestetic (x- or y-axis, colour etc.) to I pick to reprent a given dimension of the data on the plot type?\n",
    "\n",
    "The dataset is straightforward, it is the `total_spent` per invoice of a given customer. The a barplot is quite suitable and we call it with `sns.barplot()`. It comes with a few vary important parameters. The `data=` parameter is where we define the dataset that we want to plot and the `x=` and `y=` are the aestetics (or visual aides) of the plot where we **map** a choice of dimensions. Namely we want to have one bar per invoice, hence we split the `InvoiceNo` over the `x`-axis and we want the height of the bars to represent the `total_spent`.\n",
    "\n",
    "Finally we print the plot with `plt.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the barplot of the total spent per invoice number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus activity\n",
    "\n",
    "Try to order the bars when plotting them. You could again use the `.sort_values()` mehtod.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the barplot of the total spent per invoice number using an increasing order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Activity - Country spending\n",
    "\n",
    "Let's what country is spending the most. Here again `.groupby()` will come in handy as well as the function `total_spent()` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform group by country\n",
    "\n",
    "# Apply total spent in each grouped category\n",
    "\n",
    "# Resetting index\n",
    "\n",
    "# Sorting by total spent\n",
    "\n",
    "# Present the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that could easily reuse our function to produce a completely new result based on the same operation. The only thing we changed was the group - we grouped by `Country` instread of `InvoiceNo`.\n",
    "\n",
    "Let's further clean up the result in converting the `Country` index into a column for more convenient plotting and let's sort the values to a clearer message.\n",
    "\n",
    "Let's again use a barplot to visualise this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the total spent per country using a barplot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above show that clearly most of the money is pent in the UK. Perhaps it would be a good idea to use a **log-scale** here to distinguish between the lower ranking countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Repeat the plot by this time apply a log scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - A history of  purchases\n",
    "\n",
    "Up until now, you have learned a lot about the data, but we do not yet know much about what time period the dataset is spanning? To get an impression we have the `IncoiceDate` column to play with. At this point we can conveniently calculate the total_spend per time unit. Let's start with the time unit in which the data was loaded, simply on 'InvoiceDate'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform group by 'InvoiceDate'\n",
    "\n",
    "# Apply total spent in each grouped category\n",
    "\n",
    "# Display the some of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the apply was performed on a minute basis, which might not be the most useful (and takes a bit of time). \n",
    "\n",
    "Let's calcualte the `total_spent` on a monthly basis. Notice that this time we can not reset the index (with `.reset_index()`) as we need the indexed form to apply the `.resample()` method, that allows to switch between time resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use the '.resample()' method from pandas with the 'M' parameter to collect\n",
    "# the monthly total spend and '.sum()' it all up in one line\n",
    "\n",
    "# Present the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this data. This time we will not use `seaborn` but a ususal suspect, that beyond making data maniplations more convenient (hint, hint), has been developed to work very well with timeseries data. You guess right, `pandas` comes with its own plotting functionality and it is ver easy to plot time series.\n",
    "\n",
    "We simply use the `.plot()` method on the `pd.DataFrame` this time specifying that we want to use the index as our x-axis (remember, `seaborn` does not play well with `pandas` indices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the total spent per month "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our dataset spand a period from December 2010 to August 2011. We can also see that there are fluctuations over the year and also within the weeks probably suggesting particular spending patterns for weekend days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Activity\n",
    "\n",
    "Repeat the opeartion to get the `total_spent` on a daily basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the total spent per day \n",
    "# '.resample()' with the 'D' parameter and .sum() up the results before plotting the time series with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now equipped with all the tools to build a dataset that summarised all the imporant information the intial dataset that you were given. It will help to systematically understand customer behaviour. In the next modules we will use the dataset that you generated to do customer segmentation and predictions to try and classify whether customers are likely to be returning to the online shop.\n",
    "\n",
    "![Visual Representation of Dataset](img/online_retail_visual_table.png)\n",
    "\n",
    "There will be a two step building process to the dataset:\n",
    "\n",
    "* aggregation based on `InvoiceNo`\n",
    "* aggregation based on `CustomerID`\n",
    "\n",
    "![2-step Aggregation](img/online_retail_two_step_aggregation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Summarise Invoices\n",
    "\n",
    "You will start by summarising all the information contained in the invoices. To extract as much meaningful information as possible, you define custom functions that summarise different aspects of customer behaviour. This is a crucial step in any data science endvour and is called **feature engineering**.\n",
    "\n",
    "Let's start with a function that we already know: `total_spent`. We will essentially re-use the same function as before, but we need to give it a little update. This time we will want to make sure that when we look at items purchases that they have a positive `Quantity` associated with them. We can achieve this by updating the functions operation to filter for rows in the `pd.DataFrame` where `Quantity > 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is a slight modification to the total_spent function you built above\n",
    "\n",
    "def total_spent(data):\n",
    "    purchased = data[(data['Quantity'] > 0)]  # make sure to filter for rows where an item was sold (not returned)\n",
    "    spent = purchased['Quantity'] * purchased['UnitPrice']\n",
    "    return spent.sum()  # we sum up the sending per items and return the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further little tweak is concerned with the way that the function returns the computed feature. If you remember in the previous version of the function, we returned the result in a `pd.Series` object. This was so that we could assign a column name to the output. This time we want to just return the simple calculated values as we will let the naming be done by an additional convenience function, which single role is to build a neat final `pd.DataFrame` (see below).\n",
    "\n",
    "But before we get into that, let's define some more features and functions that can caluclate them. It could be interesting to see how much customers have been refunded. This function will be very simular to the the `total_spent` and we can name it `total_refunded`. The single difference is that we are this time interested in the entries of the table where there was a negative `Quantity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function is very similar to the one above, except that it records\n",
    "# the number of items that were returned (rather than sold)\n",
    "\n",
    "def total_refunded(data):\n",
    "    returned = data[(data['Quantity'] < 0)]\n",
    "    refunded = returned['Quantity'] * returned['UnitPrice']\n",
    "    return refunded.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature could be the total number of items purchases. This function will be called `total_items()` and operates in a similar way, except that it ignores `UnitPrice`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TEST ACTIVITY ### \n",
    "\n",
    "# Define a function called 'total_items' that takes data as a parameter.\n",
    "# It should filter all rows where the 'Quantity' is more than 0, i.e.\n",
    "# where an item was sold (not returned). Then it should select the \n",
    "# 'Quantities' column and return the sum.\n",
    "\n",
    "### WRITE YOUR CODE HERE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the corresponding function that is concerned with refunded items can be quickly coded up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function is very similar to the one you just write, except that it\n",
    "# records the number of items that were returned (rather than sold)\n",
    "\n",
    "def total_items_returned(data):\n",
    "    n_items = data[(data['Quantity'] < 0)]['Quantity']\n",
    "    return n_items.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, all the functions defined will just return a `pd.Series` that does not have a name associated with it. That is there the `compute_invoice_metrics` functions comes in. This function is required because with the `.apply()` method in `pandas` we can only call a single function. `compute_invoice_metrics` is a trick to circumvent this limitation in that it is a single function that calls other functions as it is called. Furthermore it naming the output from the other functions and return the result per group in a `pd.Series` that is stiched up nicely to become the final `pd.DataFrame` that is output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This functions defines what other functions (which we defined above) we want to \n",
    "# apply to the grouped dataset\n",
    "\n",
    "def compute_invoice_metrics(data):\n",
    "    result = {\n",
    "        'total_spent': total_spent(data),\n",
    "        'total_refunded': total_refunded(data),\n",
    "        'total_items': total_items(data),\n",
    "        'total_items_returned': total_items_returned(data)\n",
    "    }\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the required functions defined, we can start with the processing. First we will group the data with a `.groupby()`. This time we will use multiple columns by which we want to group. (Note: Although we know that each invoice has a unique `InvoiceNo`, we also add other columns at this stage. This only way to keep these columns as additional output in the resulting table beyond the features that our custom functions calculate. We want to keep these columns are we need to use them in the next aggregation step when we produce more features on a **per-customer-basis**.) \n",
    "\n",
    "Then we apply the `compute_metrics()` function to the grouped table. (**Note: This can take some time!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Group the invoices this time by multiple columns including\n",
    "# all of 'InvoiceNo', 'InvoiceDate', 'CustomerID' and 'Country'.\n",
    "\n",
    "# Apply the 'compute_invoice_metrics' you defined above\n",
    "\n",
    "# Reset index\n",
    "\n",
    "# Display the first rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Activity\n",
    "\n",
    "Can you design additional features that you think could be useful? Give it a go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Summarise customers\n",
    "\n",
    "You will now have a table where every row has a single `InvoiceNo` with various summaries or features associated with that number. However we are interested in better understanding our customers. So we need to apply a further summarising step to get to each row being a single `CustomerID`. Then we can learn behavioural aspects or customers.\n",
    "\n",
    "This time you will calculate a time-based feature. To be specific you will learn every how may days a customer places an order. This function is a bit more involved as it has to deal with considerations that are specific to time data. The details do not matter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function records the time that has passed between two orders\n",
    "\n",
    "def time_between_orders(data):\n",
    "    t = data[(data['total_items'] > 0)]['InvoiceDate']  # filter for the rows where an item was sold (not returned)\n",
    "    t = t.sort_values()  # make sure to sort values according to date\n",
    "    try:\n",
    "        timedelta = t - t.shift(1)  # shift by one row\n",
    "        days = timedelta.astype('timedelta64[D]')  # convert to n. days\n",
    "        days = days.mean()  # return the average time between orders (to aggregate in case there were many orders)\n",
    "    except TypeError:\n",
    "        days = np.NaN  # this covers the cases where there was only one purchase and we can not take a delta.\n",
    "    return days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we have a summarising function `compute_customer_metrics()` that will be called with the `.apply()` to in turn call all the functions that actually calculate the features. This time we mostly `.sum()` up the individual invoices' totals to get to a value that reflect the customer as a whole. Features that we calculate on a per-customer-basis should include:\n",
    "\n",
    "* total_spent\n",
    "* total_refunded\n",
    "* total_items\n",
    "* total_items_returned\n",
    "* min_spent\n",
    "* mean_spent\n",
    "* max_spent\n",
    "* balance\n",
    "* n_orders\n",
    "* time_between_orders\n",
    "\n",
    "Here you are provided with a skeleton of `compute_customer_metrics()` but it is incomplete. Add the `time_between_orders()` and also add the final `balance` per customer where it is defined as the `total_spent` - `total_refunded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function again defines what other functions we want to apply to the\n",
    "# grouped dataset, this time on a per customer basis\n",
    "\n",
    "def compute_customer_metrics(data):\n",
    "    result = {\n",
    "        'total_spent': data['total_spent'].sum(),\n",
    "        'total_refunded': data['total_refunded'].sum(),\n",
    "        'total_items': data['total_items'].sum(),\n",
    "        'total_items_returned': data['total_items_returned'].sum(),\n",
    "        'min_spent': data['total_spent'].min(),\n",
    "        'mean_spent': data['total_spent'].mean(),\n",
    "        'max_spent': data['total_spent'].max(),\n",
    "        'balance': data['total_spent'].sum() + data['total_refunded'].sum(),  \n",
    "        'n_orders': len(data),\n",
    "        'time_between_orders': time_between_orders(data)  \n",
    "    }\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again you will group the dataset that you want to aggregate, which this time is the invoices `pd.DataFrame`. And finally you apply the `compute_customer_metrics()` function you defined and reset the indices. (Note: This can again take some time, but should be faster than the aggregation before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Group by Customer ID and Country\n",
    "\n",
    "# Apply the compute_customer_metrics() function in the grouped instances\n",
    "\n",
    "# Display the first rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last you have gone from an item-based table to an information rich customer-based table that you can now supply to unsupervised and supervised machine learning algorithms to try and learn something about the customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is just *one* more thing... *Quality control*. We can immediately see that for instance for the `time_between_orders` column we have some missing data as the `count` value is lower than that of the other columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Imputation\n",
    "\n",
    "We should first take care to replace the missing values in the dataset as they prevent machine learning algorithms to run. There are many strategies to help with missing data and they depend on whether the missing data is numeric or categorical.\n",
    "\n",
    "* simply removing rows where there is missing data (e.g. `.dropna()` can achieve this)\n",
    "* imputing the values with a summary statistic such as mean or median or most frequent value (e.g. `Imputer` from `sklearn` module)\n",
    "* replace the values with a resonable estimate\n",
    "\n",
    "What strategy is best for you problem very much depends on the specifics of your dataset. However generally speaking it is not worth to remove large chunks of data.\n",
    "\n",
    "In our case the missing values are exclusively found in the `time_between_orders` column, so we should have a look at these rows where this occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the instances where the time_between_orders is empty\n",
    "\n",
    "# How many nan cases do we have? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eyeballing the table we can see that only in the rows where we have a single order (`n_orders == 1`) that the `time_between_orders` is `NaN`, i.e. Not a Number. That makes immediate sense and indicates that these customers have not yet returned for anther purchase.\n",
    "\n",
    "A reasonable strategy here would be to replace the `NaN` values by the longest time period (in days) that we would expect a customer to be returning, e.g. 365 days. So let's replace all the `NaN` values with the `pandas` method `.fillna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Take the empty values (NAs) from the column 'time_between_orders' and fill them with the value 365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Removing Outliers\n",
    "\n",
    "Now let's move on to taking care of the blatant outliers. An outlier is an observation that appears extreme relative to the rest of the data. Some ML techniques are sensitive to outliers and it's better to remove these samples before proceeding.\n",
    "\n",
    "There are again many strategies to deal with this scenario. The real question that we need to answer is at what point we consider a value extreme and whether it is really legitimate to remove it from the observations.\n",
    "\n",
    "Here we have defined a simple function that provides for a straighforward way of removing observations that are `k` standard deviations (`sigma`) away from the mean (`mu`) of a distribution.\n",
    "\n",
    "Assuming that the data is normally distributed, approximately 99.7% (almost everything) of the data falls within three standard deviations of the mean. \u001bUnder this assumption we are considering as outliers only samples with very unlikely values for a features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function defines what datapoints we consider \n",
    "\n",
    "def remove_outliers(data, k=3):\n",
    "    mu = data.mean()  # get the mean\n",
    "    sigma = data.std()  # get the standard deviation\n",
    "    filtered_data = data[np.abs((data - mu) / sigma) < k]  # filter values based on distance from mean\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can `.apply()` this function. In case that the value is decalred an outlier, its value is replace by `NaN`, keeping the structure of the `pd.DataFrame` intact. However it only operates on numerical columns. Therefore, we first need to some `pd.DataFrame` processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply the remove_outliers function to the customers dataframe and display the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now remove the `NaN` values. This can easily be achieve with the `.dropna()` method that takes care of all the rows with a single occurence of `NaN` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Scaling\n",
    "\n",
    "Often when we are working with multidimensional data, the data have different units and thus exist on different scales. When comparing the data internally, or when the values map to similar space, then that is not a problem. However if a dimensions is in the millions, e.g. population of a country, and an others dimension in the few hundreds, e.g. number of hospitals, then there can be an uneven impact of the dimensions with higher values.\n",
    "\n",
    "You can easily visualise this with a `boxplot`. `boxplot` represent essential statistics that describe distributions; from bottom to top, the horizontal lines of the box represent the first quartile (`Q1`), the median and third quartile (`Q3`). The distance between `Q1` and `Q3` is called inter quartile range (`IQR`). The whiskers of the boxes on the top and bottom are defined as `Q1 - 1.5 x IQR` and `Q3 - 1.5 x IQR` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot a sns.boxplot() of the customer dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance you can see that `n_orders` is defined in a much narrower space than `balance`. If you were to use the data in an unscaled form, the effect of `balance` might be disproportionnaly high.\n",
    "\n",
    "To account for this you can scale your data, so that all the dimensions fall onto the same space. We use a simple function from the `sklearn` library for this purpose. Namely we use the `StandardScaler()`. In the coming sections of the bootcamp we will be using `sklearn` extensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialise the scaler\n",
    "\n",
    "# Apply auto-scaling (or any other type of scaling) and cast to DataFrame \n",
    "\n",
    "# Print the first rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replot the `boxplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Replot the boxplot with the scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Learning Activity - Investigate the relationship between input features\n",
    "\n",
    "Visualisation is an integral part of Data Science. Exploratory data analysis (EDA) is the field dealing with the analysis of data sets as a means of summarising their main characteristics, most often using visual methods.\n",
    "\n",
    "One of the most powerful tools commonly used as part of EDA is the scatter plot. This step helps visualise the relationship in-between two input features, indicates the degree of correlation plus helps reveal any patterns and trends in the data, and may also give you a first indication of the ML model that could be applied and its complexity (linear vs. non-linear). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot of the first two features 'balance' vs. 'total_spent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Activity - Create a scatterplot of different features\n",
    "\n",
    "Try once more creating an enhanced scatterplot as before, but this time using the features 'balance' and 'total_spent'. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an enhanced scatter plot of the features 'balance' vs. 'total_spent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Activity - Try different combinations of the input features in a grid/scatterplot matrix\n",
    "\n",
    "A scatterplot matrix shows a grid of scatterplots where each attribute is plotted against all other attributes. You can find further information on how to create a scatterplot matrix with seaborn using the pairplot() function at https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.pairplot.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a scatterplot matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Create a correlation matrix and heatmap of correlations between the input features\n",
    "\n",
    "It is often of great interest to investigate whether any of the variables in a multivariate dataset are significantly correlated. As we previously demonstrated, the different features (variables) in `customers` are not completely independent from each other. To quickly identify which features are related and to\n",
    "what degree, it is useful to to calculate a correlation matrix that shows the correlation coefficient for each pair of variables. You can do this by using the `corr()` function from the `pandas` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the correlation coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for linear relationships between features across all pairs of features, you can use a heatmap\n",
    "of correlations, which is simply a matrix of subplots whose colours represent the\n",
    "degree of the correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a heatmap of the correlation coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Activity: Mapping Categorical Features\n",
    "\n",
    "As previously mentioned, most machine learning libraries and tools will only accept numerical values as their input. In the case where we have categorical features present, we need to represent them as numerical values. Unlike the label encoding we demonstrated previously for the class vector, when dealing with categorical input features, typically one converts each categorical feature using “one-hot encoding”. The input in one-hot encoding is the vector of discrete categorical values, and the output will be a sparse matrix where each column corresponds to one possible value of one feature.\n",
    "\n",
    "In our example, the feature Country is a categorical feature with values such as [\"Finland\", \"Norway\", \"Spain\"] etc. Such features can be easily mapped to dummy variables which could be expressed as [0, 1, 2]. Can you spot any problem though with this approach? Even though the country values do not come in any particular order, a machine learning algorithm will now assume that \"Spain\" is larger than \"Norway\", and \"Norway\" is larger than \"Finland\", and so on. Although this assumption is incorrect, the algorithm could still produce useful results. However, those results would not be optimal.  \n",
    "\n",
    "The correct approach in this case is to apply one-hot encoding. This estimator transforms each unique categorical value of a single input categorical feature to a new dummy **feature**. So, for our 11 unique country values, we will end up with 10 new dummy features after one-hot-encoding. \n",
    "\n",
    "There are plenty of libraries and functions that are used for one-hot encoding. In this example, we will use the `get_dummies()` function from `pandas`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to the categorical feature Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we want to update in our *customers* and *X* Dataframes the categorical feature \"Country\" with the result of one-hot encoding. Remember, that are going to remove a single feature, and add 11 new dummy variables created by one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove and replace the Country values from customers with the result of one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3\n",
    "\n",
    "The amount of data generated each day has been growing exponentially over the past years. This may result in the generation of a large number of input features, a subset of which may be highly correlated, repetitive, not very informative or even related to the particular study. Datasets with a large number of features are commonly referred to as high-dimensional datasets.\n",
    "\n",
    "In this module, we will investigate Principal Component Analysis (PCA) as a technique for dimensionality reduction, data compression and feature extraction. The PCA algorithm reduces the initial number of possibly correlated variables into a new lower number of uncorrelated (orthogonal) variables, known as the Principal Components (PCs). PCA is commonly applied prior to unsupervised and supervised machine learning models to reduce the number of features used in the analysis, thereby reducing the likelihood of error.\n",
    "\n",
    "\n",
    "## Principal Component Analysis \n",
    "\n",
    "Principal Component Analysis (PCA) is the most commonly used technique for dimensionality reduction, data compression and feature extraction. The PCA algorithm reduces the initial number of possibly correlated variables into a new lower number of uncorrelated variables, more specifically in a set of successive orthogonal components that explain a maximum amount of the variance, known as the Principal Components (PCs). \n",
    "\n",
    "### Learning Activity - Apply PCA in the input data using scikit-learn\n",
    "\n",
    "In scikit-learn, PCA is implemented as a transformer object that learns _n_ components in its `fit()` method, and can be used on new data to project it on these components. More information on how to use the `pca()` function and its parameters can be found at http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialise the PCA object and create an index for each Principal Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the Principal Components (scores) can be computed by the `fit_transform()` (alternatively, `fit()` followed by the `transform()`) function. This function returns a matrix with the principal components, where the first column in the matrix contains the first principal component, the second column the second component, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the PCA scores matrix and check the dimensionality of the PCA scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loadings for the principal components are stored in a named element *components_*. This contains a matrix with the loadings of each principal component, where the first column in the matrix contains the loadings for the first principal component, the second column contains the loadings for the second principal component, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the PCA loadings matrix and check the dimensionality of the PCA loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity: Calculate and plot the explained and cumulative variance \n",
    "\n",
    "But how much information have we lost? We can figure this out by looking at the explained and cumulative variance. The explained variance gives us the proportion of variance explained by each successive Principal Component. The cumulative variance  is obtained by adding the successive proportions of explained variance to obtain the total sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the explained and cumulative variance\n",
    "\n",
    "# Calculate the explained variance\n",
    "\n",
    "# Calculate the cumulative variance\n",
    "\n",
    "# Combine both in a data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the explained variance using a barplot with seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the explained variance per PC using a barplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Activity - Plot the cumulative variance\n",
    "\n",
    "Plot the cumulative variance using a line plot, screeplot or barplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the cumulative variance per PC using a barplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Plot the PCA loadings using a heatmap\n",
    "\n",
    "Each Principal Component is a linear combination of all the variables and is perpendicular to every other component. Each variable in each component is multiplied by set of factors, the loading factors, which transforms the original data into this new component space. These loading factors serve as weights to see which parameters are most important for a particular principal component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the PCA loadings in a heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this heatmap to see how the different features load onto the different components. For example, PC1 seems to be concerned mainly with volume of purchase (hence the heavy loadings of the item quantity and spending amounts), while PC 3 seems to be mainly concerned with the total refunded. In real life scenarios, these components can often be attributed to higher level concepts with respect to the features (e.g. length, breadth, height might form a \"size\" component) or domain-specific constructs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Create a scatterplot of the first two PCs\n",
    "\n",
    "Often PCA scores plots are used in the hope that they will reveal clusters, trends and patterns in the data.\n",
    "So, let us start by creating a scatterplot of the first two principal components (scores of PC1 and PC2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a scatterplot of the first two Principal Components\n",
    "# Remember, we will use scores (!) rather than customers as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Activity - Reading in the associated classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we will import and join per row with the `customers` DataFrame the associated classes in order to use this knowledge during visualisation. Try to import the customer classes from the provided \"`customer_classes.csv`\" into the variable **_y_**. Remember to also define the column that will be used as the row labels of the DataFrame as in the previous step (if you are unsure, open the csv file and try to decide the column name you are going to use). \n",
    "\n",
    "In this case, the class **_y_** contains two classes (binary case) - \"yes\" vs. \"no\" - that represent the returning and non-returning customers respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the data from the customer_classes.csv, set the index column and explore the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to check the dimensionality of your input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the dimensionality of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Joining with class\n",
    "\n",
    "A very useful feature of `pandas` is its `join()` function, which allows combining tables based on one column shared between tables. Here we use `join()` to combine the input features and information on whether customers return (associated classes).\n",
    "\n",
    "You will join the class labels with the dataset by a shared index. **`CustomerID`** is the obvious choice here, as it is the only column shared between the two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join the \"customers\" and y DataFrames based on the common CustomerIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join the \"scores\" (PCA) and y DataFrames based on the common CustomerIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity: Create enhanced PCA scores plot\n",
    "\n",
    "It is often quite useful to create a scatterplot of the first two principal components (Xscores of PC1 and PC2), colour the observations based on their associated class labels, and see whether we can detect any patterns or well-defined clusters. \n",
    "\n",
    "Create an enhanced graph of the first two PCs, but this time use as plotting data the `scores_enhanced` dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an enhanced scatterplot of the first two Principal Components\n",
    "# Remember, we are using scores_enhanced (!) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot shows PC1 (1st Principal Component) on the x-axis, and PC2 (2nd Principal Component) on the y-axis. Based on this plot, we can see that the observations of the two classes are highly overlapping in 2D space, therefore, it may be interesting to investigate the first 3 PCs using a 3D scatterplot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Activity: Create a 3D PCA scores plot using seaborn or Plotly\n",
    "\n",
    "We may want to investigate simultaneously the first three Principal Components (PCs) using seaborn or Plotly. *Hint* you may need to use a for loop to iterate through each of the available classes while building the scatterplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a 3D scatterplot of the first three Principal Components "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Export to csv\n",
    "\n",
    "Now that we have produced the datasets that are ready for applying some machine learning algorithms we will save (or \"export\") them to our local machine and working repository. This also served as a checkpoint for the bootcamp so that you can get started straight away with the next module even if you got stuck with some part above.\n",
    "\n",
    "Writing a `pd.Dataframe` to disk is very easy - you just use the `.to_csv()` method, and specify the file path to where you want it saved. There also other [formats](http://pandas.pydata.org/pandas-docs/stable/api.html#id12) that you save to, which are based on functions that work in exactly the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Save to a csv file with the '.to_csv()' method and give the file a name you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END OF DAY 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
