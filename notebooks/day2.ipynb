{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 \n",
    "\n",
    "## Module 4\n",
    "\n",
    "\n",
    "### Learning Activity - Loading Libraries\n",
    "\n",
    "We will start once more by loading the required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import warnings\n",
    "\n",
    "# Module 4\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics.cluster import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Module 5\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D \n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "\n",
    "%matplotlib inline\n",
    "init_notebook_mode()\n",
    "warnings.simplefilter(action = \"ignore\")\n",
    "\n",
    "print(\"libraries all imported, ready to go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity: Reading the data\n",
    "\n",
    "As a first step, we will need to import the data from the \"`retail_ml_dataset.csv`\" data file that we constructed and exported on Day 1 (or the backup file \"`retail_ml_dataset_backup.csv`\" that we have provided) into the variable **_X_** using the `read_csv()` function from `pandas` (`pd`). We also want to define the column that we are going to use as the row labels of the DataFrame; in this case, *'CustomerID'*. Once loaded, we can apply once more the `head()` function to preview the first 5 rows of our DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the data from the retail_ml_dataset.csv, set the index column and explore the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, it is **always** a good practice to check the dimensionality of the imported data using the `shape` command prior to constructing any classification model to make sure you have correctly imported all the data (e.g. one common mistake is to get the separator wrong and end up with only one column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the dimensionality of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out the non-binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the data from the pca_scores.csv, set the index column and explore the first few rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with K-Means\n",
    "\n",
    "K-means clustering is a method for finding clusters and cluster centers in a set of unlabeled data. Intuitively, we might think of a cluster as comprising a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster. Given an initial set of K centers, the K-means algorithm alternates the two steps:\n",
    "\n",
    "1. for each center we identify the subset of training points (its cluster) that is closer to it than any other center;\n",
    "2. the means of each feature for the data points in each cluster are computed, and this mean vector becomes the new center for that cluster.\n",
    "\n",
    "These two steps are iterated until the centers no longer move or the assignments no longer change. Then, a new point x can be assigned to the cluster of the closest prototype.\n",
    "\n",
    "### Learning Activity - Run K-Means with two features\n",
    "Isolate the features `mean_spent` and `max_spent`, then run the K-Means algorithm on the resulting dataset using K=2 and visualise the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Appy k-means with 2 clusters using a subset of features (mean_spent and max_spent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function generates a pairplot enhanced with the result of k-means\n",
    "\n",
    "def pairplot_cluster(df, cols, cluster_assignment):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        df, dataframe that contains the data to plot\n",
    "        cols, columns to consider for the plot\n",
    "        cluster_assignments, cluster asignment returned by the clustering algorithm\n",
    "    \"\"\"\n",
    "    # seaborn will color the samples according to the column cluster\n",
    "    df['cluster'] = cluster_assignment \n",
    "    sns.pairplot(df, vars=cols, hue='cluster')\n",
    "    df.drop('cluster', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise the clusters using pairplot_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separation between the two clusters is neat (the two clusters can be separated with a line). One cluster contains customers with a low spendings and the second with high spendings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Activity - Run K-Means with all the features\n",
    "Run K-Means using all the features available and visualise the result in the subspace `mean_spent` and `max_spent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Appy k-means with 2 clusters using all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise the clusters using pairplot_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is now different. The first cluster contains customers with a maximum spending close to the minimum mean spending and the second contains customers with a maximum spending far from the minimum mean spending. This way can tell apart customers that could be willing to buy object that cost more than their average spending.\n",
    "\n",
    "***Question***: Why can't the clusters be separated with a line as before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning activity - Compare expenditure between clusters\n",
    "\n",
    "Select the features `'mean_spent'` and `'max_spent'` and compare the two clusters obtained above using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare expenditure between clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Compare mean expediture with box plot\n",
    "\n",
    "Compare the distribution of the feature `mean_spent` in the two clusters using a box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare mean expediture with box plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Compare the mean expenditure distributions\n",
    "\n",
    "User the function `distplot` to show the distribution of the mean expen in both clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare the mean expenditure distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Activity - Looking at the centroids\n",
    "\n",
    "Look at the centroids of the clusters `kmeans.cluster_centers_` and check the values of the centers in for the features `'mean_spent', 'max_spent'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare the centroids "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we note:\n",
    "- Cluster 0 contains more customers.\n",
    "- Customers in cluster 1 spend more in average, but have a more changeble behaviour.\n",
    "- Customers in cluster 1 place more order and ask for more refunds.\n",
    "\n",
    "We can study the averages also looking at the centroids:\n",
    "\n",
    "***K-Means, pro and cons***\n",
    "\n",
    "Pro\n",
    "- fast, if your dataset is big K-Means might be the only option\n",
    "- easy to understand\n",
    "- any unseen point can be assigned to the claster with the closest mean to the point\n",
    "- many implementsions available\n",
    "\n",
    "Cons\n",
    "- you need to guess the number of clusters\n",
    "- custers can be only globular\n",
    "- the results depends on the initial choice of the means\n",
    "- all the points are assigned to a cluster, clusters are affected by noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Compute the silhouette score\n",
    "Compute the silhouette score of the clusters resuting from the application of K-Means.\n",
    "\n",
    "The Silhouette Coefficient is calculated using the mean intra-cluster distance (``a``) and the mean nearest-cluster distance (``b``) for each sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a, b)``. It represents how similar a sample is to the samples in its own cluster compared to samples in other clusters.\n",
    "\n",
    "The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the silhouette score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette score is pretty high, we can say that the clusters are compact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Activity - Run KMeans on the dataset obtained with PCA\n",
    "\n",
    "Compute KMeans on the dataset `XScores` usgin the first 2 principal components.\n",
    "\n",
    "Visualize the results using again the function `pairplot_cluster` in the first 4 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run KMeans on the first two principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering: Linking with Linkage\n",
    "\n",
    "The main idea behind hierarchical clustering is that you start with each point in it's own cluster and then\n",
    "\n",
    "1. compute distances between all clusters\n",
    "2. merge the closet clusters\n",
    "\n",
    "Do this repeatedly until you have only one cluster.\n",
    "\n",
    "This algorithm groups the samples in a bottom-up fashion and falls under the category of the agglomerative clustering algorithms.\n",
    "\n",
    "According to the distance between clusters and samples that one choose the clusters will have different properties. In this section we'll use a distance that will minimizes the variance of the clusters being merged.\n",
    "\n",
    "This algorithm results in a hierarchy, or binary tree, of clusters branching down to the last layer which has a leaf for each point in the dataset that can be visualise with a \"Dendrogram\". The advantage of this approach is that clusters can grow according to the shape of the data rather than being globular.\n",
    "\n",
    "sklearn implements hierarchical clustering in the class `sklearn.cluster.AgglomerativeClustering` (http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering), this class is mainly a wrapper to the functions in `scipy.cluster.hierarchy` (http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Plotting dendograms\n",
    "Use the function `linkage()` from `scipy.cluster.hierarchy` to cluster the retail data and pass the result to the function `dendrogram()` to visualise the result. Trunc the dendrogram if the initial result is unreadable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply hierarchical clustering \n",
    "\n",
    "# Draw the dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coloring of the figure highlights that the data can be segmented in two big clusters that were merged only in the very last iterations of the algorithm. But, if we look close, we can spot another smaller cluster that was merged to the red one at a distance of around 40.\n",
    "\n",
    "We can improve the readability of the dendrogram showing only the last merged clusters and a threshold to color the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the dendrogram using a cut_off value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we able to see the threshold that reflects the color to the clusters and we easily realise that the closer it is to zero, the more clusters are highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Running Linkage with Sklearn\n",
    "\n",
    "Use `sklearn.cluster.AgglomerativeClustering` to cluster the retail data according to the 3 clusters highlighted by the dendrogram above and visualise the result in the subspace give by the features `mean_spent` and `max_spent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform clustering with AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise the clusters using pairplot_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is similar to the one we had with K-Means, but now we also tell apart customers that moderately deviate from their average with their maximum spenging and customer that strongly deviate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Visualising the clusters in 3D\n",
    "Create a 3D chart where the results of the Linkage algorithm is shown in the space formed by the features `min_spent`, `max_spent` and `mean_spent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function generates a 3D plot enhanced with the result of clustering\n",
    "\n",
    "def scatter_cluster3d(x, y, z, cluster_assignment, fig):\n",
    "    ax = Axes3D(fig)\n",
    "    for cluster in np.unique(cluster_assignment):\n",
    "        ax.scatter(x[cluster_assignment==cluster], \n",
    "                   y[cluster_assignment==cluster], \n",
    "                   z[cluster_assignment==cluster],\n",
    "                   c=sns.color_palette()[cluster], label='cluster '+str(cluster))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the clusters in 3D using the scatter_cluster3d() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this space we can see that cluster 1 has less variablity respect to cluster 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Activty - Interactive 3D visualisation with Plotly\n",
    "\n",
    "Recreate the 3D plot above with Plotly.\n",
    "\n",
    "Here are some example to inspire your code: https://plot.ly/python/3d-scatter-plots/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create an interactive 3D plot enhanced with the result of clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can navigate this chart with you mouse and hide/show the cluster clicking on the legend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hierarchical clustering, pro and cons***\n",
    "\n",
    "Pros\n",
    "- The clusters are not globulars anymore\n",
    "- Doesn't depend on initial random choices\n",
    "- The dendrogram shows a good summary of how the algorithm works\n",
    "\n",
    "Cons\n",
    "- Slower than K-Means\n",
    "- We still need to choose the number of clusters\n",
    "- Still, the clusters are affected by noisy points\n",
    "- Assigning a new point to a cluster is not straightforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, min_samples and eps, which define formally what we mean when we say dense. Higher min_samples or lower eps indicate higher density necessary to form a cluster. \n",
    "\n",
    "Summary of the Algorithm:\n",
    "\n",
    "- starts with an arbitrary starting point and retrieved all the points in the radius of distance `eps` from it \n",
    "    - if the radius contains `min_samples` points, start a cluster\n",
    "      - add all the points in the radius of distance `eps` to the cluster and their `eps` neighbors.\n",
    "      - continue expanding the cluster iterating on the the procedure on all the neighbors\n",
    "    - otherwise mark it as noise/outlier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sklearn implementation doc: http://scikit-learn.org/stable/modules/clustering.html#dbscan\n",
    "\n",
    "Animated DBSCAN: http://www.naftaliharris.com/blog/visualizing-dbscan-clustering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Activity - A starting value for eps\n",
    "\n",
    "Measure the distance of each point to its closest neighbor using the function `sklearn.metrics.pairwise.pairwise_distances` (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html) and plot the distribution of the distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the distance of each point to its closest neighbor\n",
    "\n",
    "# Plot the distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the distance will help us choose a starting point for `eps`. We see that it's very likely that a point as at least one neighbour in a radius of 0.15 and that only very few point have it at distance 2.5. Since we want that a core point has more than one point in is `eps`-neighborhood we can start picking `eps` on the right tail of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Activity - Applying DBSCAN\n",
    "\n",
    "Cluster the customer data with DBSCAN and visualise the results in the subspaces used for the other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise the clusters using pairplot_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN clustered all the points in one big cluster and marked as outiers all the points that are not in dense areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise the clusters in 3D using the scatter_cluster3d() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that DBSCAN grouped most of the samples in 1 big cluster and maked samples at the border of this space as outliers. Which means that DBSCAN acted as an outlier detection algorithm more than a clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Activity - Run DBSCAN on the dataset obtained with PCA\n",
    "\n",
    "Run DBSCAN on the first 3 principal components in `Xscores`.\n",
    "\n",
    "Tweak the parameters to achieve about 5 clusters as result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run DBSCAN on the first 3 principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning activity - Compute the silhouette score of the DBSCAN cluster\n",
    "\n",
    "Compute the silhouette score of the clusters made with DBSCAN and compare it with the silhouette score achieved with K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the silhouette score of DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity -  How many clusters with DBSCAN?\n",
    "\n",
    "Vary `eps` and `min_samples` and study how the number of clusters varies as result. This way we'll have an idea of how many cluster we get varying the parameters. This can help us choose the parameters if we already have an idea of how many clusters we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WARNING this may take a couple of minutes to finish!\n",
    "eps = np.linspace(.5, 2.0, 20)\n",
    "mins = np.arange(10, 50, 5)\n",
    "Z = np.zeros((len(eps), len(mins)))\n",
    "\n",
    "for i, e in enumerate(eps):\n",
    "    for j, m in enumerate(mins):\n",
    "        db = DBSCAN(eps=e, min_samples=m)\n",
    "        clusters_found = len(np.unique(db.fit_predict(customers)))\n",
    "        Z[i,j] = clusters_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot in a heatmap "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DBSCAN, pro and cons***\n",
    "\n",
    "Pros\n",
    "- The clusters are not globulars anymore\n",
    "- We don't have to chose the number of clusters\n",
    "- Fast, few clustering algorithms can tackle datasets as large as DBSCAN can.\n",
    "- Has an embedded concept of noise (outliers)\n",
    "\n",
    "Cons\n",
    "- `eps` and `min_samples` can be hard to tune\n",
    "- less intuitive than K-Means or Linkage\n",
    "- assigning an unseen sample to a cluster is not straightforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 5\n",
    "\n",
    "This module will focus on introducing, building and optimising a classification model, the K Nearest Neighbor (KNN) classifier. The principle behind KNN is very simple. Given a dataset where you know the class labels, when a new point is introduced, you want to find a particular number of points in the data closest in distance to the new point. These are called the “nearest neighbours”.  You then use the labels associated with these nearest neighbour points (which may or may not be different from each other) to predict the label of the new point using a majority vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity: Convert DataFrames to numpy arrays\n",
    "\n",
    "In order to feed the data into our classification models in scikit-learn, the imported DataFrames need to be converted into `numpy` arrays. For more information on numpy arrays, read the documentation at http://scipy-lectures.github.io/intro/numpy/array_object.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to numpy array and check the dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Investigate the y frequencies\n",
    "\n",
    "An important aspect to understand before applying any classification algorithm is how the output labels are distributed. Are they evenly distributed or not? Imbalances in distribution of labels can often lead to poor classification results for the minority class even if the classification results for the majority class are very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate and print the y frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the class frequencies is a good way to get a feel for how the data is distributed. As a simple example, try plotting the frequencies of the class labels (held in yFreq), \"yes\" and \"no\", and see how they are distributed using a barplot from seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the y frequencies in a barplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity: Encode categorical values\n",
    "\n",
    "In our current dataset, you can see that the y values are categorical (i.e. they only take one of a discrete set of values) and have a non-numeric representation, \"yes\" vs. \"no\". This can be problematic for scikit-learn and plotting functions in Python, since they assume numerical values, so we need to map the text categories to numerical representations using LabelEncoder() and the fit_transform() function from the preprocessing module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the categorical values into numbers using the label encoder\n",
    "\n",
    "# Initialise a LabelEncoder object\n",
    "\n",
    "# Fit label encoder, return encoded labels and assign back to the class column of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate and print the y frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the categorical values have been mapped to numeric values based on their alphabetic order ('no' to 0 and 'yes' to 1). More information on LabelEncoder(), its parameters and how to use it can be found at http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Split the data into training and test sets (raw data prior to PCA)\n",
    "\n",
    "Training and testing a classification model on the same dataset is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data (poor generalisation). To use different datasets for training and testing, we need to split the wine dataset into two disjoint sets: train and test (**Holdout method**) using the `train_test_split` function. <br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the raw data into training and test sets\n",
    "\n",
    "# Print the dimensionality of the individual splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Train, optimise and test a KNN algorithm with scikit-learn\n",
    "\n",
    "To build KNN models using scikit-learn, you will be using the `KNeighborsClassifier` object, which allows you to set the value of K using the `n_neighbors` parameter (http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). For every classification model built with scikit-learn, we will follow four main steps: \n",
    "1) **Building** the classification model (using either default, pre-defined or optimised parameters), \n",
    "2) **Training** the model, \n",
    "3) **Testing** the model, and \n",
    "4) **Performance evaluation** using various metrics. <br/> <br/>\n",
    "\n",
    "The optimal choice for the value K is highly data-dependent: in general a larger K suppresses the effects of noise, but makes the classification boundaries less distinct. Rather than trying one-by-one predefined values of K, we can automate this process. The scikit-learn library provides the grid search function `GridSearchCV` (http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html), which allows us to exhaustively search for the optimum combination of parameters by evaluating models trained with a particular algorithm with all provided parameter combinations. Further details and examples on grid search with scikit-learn can be found at http://scikit-learn.org/stable/modules/grid_search.html. You can use the `GridSearchCV` function with the validation technique of your choice (in this example, 10-fold cross-validation has been applied) to search for a parametisation of the KNN algorithm that gives a more optimal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grid search with 10-fold cross-validation using a dictionary of parameters\n",
    "\n",
    "# Create the dictionary of given parameters\n",
    "\n",
    "# Optimise and build the model with GridSearchCV\n",
    "\n",
    "# Report the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process (XTest). <Br/>\n",
    "So, we are testing our independent XTest dataset using the optimised model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the classifier using the optimal parameters detected by grid search\n",
    "\n",
    "# Fit to the training set \n",
    "\n",
    "# Predict the test data\n",
    "\n",
    "# Report the final overall accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Activity - Split the PCA scores and class labels into training and test sets\n",
    "\n",
    "\n",
    "At this stage, we want to repeat the whole model-building process, starting by applying holdout like before, but this time apply it to the data generated by PCA (PCA scores) and their associated class labels. As before, you need to use the `train_test_split()` function, and remember to keep the same seed (`random_state=1`) for direct and fair comparison! You may need to select the number of PCs from your Xscores that you will feed into your model, or create a for loop to try and test different values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the results of PC scores into training and test sets\n",
    "\n",
    "# Print the dimensionality of the individual splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Train, optimise and test a KNN algorithm with scikit-learn\n",
    "\n",
    "Repeat the process of training and optimising a KNN classifier as before, but this time, apply it to your newly created train and test data (after PCA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grid search with 10-fold cross-validation using a dictionary of parameters\n",
    "\n",
    "# Create the dictionary of given parameters\n",
    "\n",
    "# Optimise and build the model with GridSearchCV\n",
    "\n",
    "# Report the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the classifier using the optimal parameters detected by grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF DAY 2 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
